# Guide de D√©pannage - D√©ploiement Kubernetes CI/CD

## Table des Mati√®res

1. [Probl√®mes Courants](#probl√®mes-courants)
   - [√âchecs de Build et Tests](#√©checs-de-build-et-tests)
   - [Probl√®mes Docker](#probl√®mes-docker)
   - [√âchecs de D√©ploiement Kubernetes](#√©checs-de-d√©ploiement-kubernetes)
   - [Probl√®mes de Health Check](#probl√®mes-de-health-check)
   - [Probl√®mes de Certificats TLS](#probl√®mes-de-certificats-tls)
   - [Probl√®mes de Secrets](#probl√®mes-de-secrets)
2. [Commandes de Debugging](#commandes-de-debugging)
3. [Proc√©dures de D√©ploiement Manuel](#proc√©dures-de-d√©ploiement-manuel)
4. [Proc√©dures de Rollback](#proc√©dures-de-rollback)
5. [Utilisation de Lens pour le Debugging](#utilisation-de-lens-pour-le-debugging)
6. [R√©cup√©ration apr√®s Incident](#r√©cup√©ration-apr√®s-incident)
7. [FAQ](#faq)

---

## Probl√®mes Courants

### √âchecs de Build et Tests

#### Probl√®me: Le workflow GitHub Actions √©choue √† l'√©tape de build

**Sympt√¥mes**:
- ‚ùå Job "build-and-test" √©choue
- Erreur: `npm run build` retourne un code d'erreur
- Message: "Build failed" dans les logs GitHub Actions

**Causes possibles**:
1. Erreurs de syntaxe dans le code
2. D√©pendances manquantes ou incompatibles
3. Variables d'environnement manquantes
4. Erreurs TypeScript

**Solutions**:

```bash
# 1. Tester le build localement
npm install
npm run build

# 2. V√©rifier les erreurs TypeScript
npm run type-check

# 3. V√©rifier le linting
npm run lint

# 4. Nettoyer et r√©installer les d√©pendances
rm -rf node_modules package-lock.json
npm install
npm run build
```

**V√©rification dans Lens**: N/A (probl√®me avant le d√©ploiement)

---

#### Probl√®me: Les tests √©chouent dans le pipeline

**Sympt√¥mes**:
- ‚ùå Job "build-and-test" √©choue √† l'√©tape de test
- Message: "Tests failed" avec des d√©tails de tests

**Causes possibles**:
1. Tests cass√©s par des changements r√©cents
2. Tests flaky (instables)
3. D√©pendances de test manquantes

**Solutions**:

```bash
# 1. Ex√©cuter les tests localement
npm test

# 2. Ex√©cuter les tests en mode watch pour debugging
npm test -- --watch

# 3. Ex√©cuter un test sp√©cifique
npm test -- path/to/test.test.ts

# 4. Voir la couverture de tests
npm test -- --coverage
```

**Pr√©vention**:
- Toujours ex√©cuter les tests localement avant de pusher
- Utiliser des hooks pre-commit pour automatiser les v√©rifications

---

#### Probl√®me: Erreur "ENOSPC: System limit for number of file watchers reached"

**Sympt√¥mes**:
- Erreur lors du build ou des tests
- Message contenant "ENOSPC" ou "file watchers"

**Solution (Linux)**:

```bash
# Augmenter la limite de watchers
echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
```

---

### Probl√®mes Docker

#### Probl√®me: √âchec du build Docker

**Sympt√¥mes**:
- ‚ùå Job "build-and-push-image" √©choue
- Erreur: "docker build failed"
- Message d'erreur dans les logs Docker

**Causes possibles**:
1. Erreurs dans le Dockerfile
2. Fichiers manquants (non copi√©s)
3. Probl√®mes de permissions
4. Manque d'espace disque

**Solutions**:

```bash
# 1. Tester le build Docker localement
docker build -t portfolio:test .

# 2. Build avec logs d√©taill√©s
docker build --progress=plain -t portfolio:test .

# 3. V√©rifier le contexte de build
docker build --no-cache -t portfolio:test .

# 4. Nettoyer les images inutilis√©es
docker system prune -a

# 5. V√©rifier l'espace disque
df -h
docker system df
```

**V√©rification du Dockerfile**:

```dockerfile
# V√©rifier que toutes les √©tapes sont correctes
# Stage 1: Dependencies
FROM node:18-alpine AS deps
WORKDIR /app
COPY package*.json ./
RUN npm ci

# Stage 2: Builder
FROM node:18-alpine AS builder
WORKDIR /app
COPY --from=deps /app/node_modules ./node_modules
COPY . .
RUN npm run build

# Stage 3: Runner
FROM node:18-alpine AS runner
WORKDIR /app
ENV NODE_ENV production
COPY --from=builder /app/.next/standalone ./
COPY --from=builder /app/.next/static ./.next/static
COPY --from=builder /app/public ./public
EXPOSE 3000
CMD ["node", "server.js"]
```

---

#### Probl√®me: Impossible de pusher l'image vers le registry

**Sympt√¥mes**:
- ‚ùå Job "build-and-push-image" √©choue √† l'√©tape push
- Erreur: "unauthorized" ou "access denied"
- Message: "failed to push image"

**Causes possibles**:
1. Credentials incorrects ou expir√©s
2. Permissions insuffisantes
3. Registry inaccessible

**Solutions**:

```bash
# 1. V√©rifier les credentials localement
# Pour GitHub Container Registry:
echo $GHCR_TOKEN | docker login ghcr.io -u USERNAME --password-stdin

# Pour Docker Hub:
echo $DOCKER_PASSWORD | docker login -u $DOCKER_USERNAME --password-stdin

# 2. Tester le push manuellement
docker tag portfolio:test ghcr.io/username/portfolio:test
docker push ghcr.io/username/portfolio:test
```

**V√©rification des Secrets GitHub**:
1. Aller sur GitHub ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions
2. V√©rifier que `GHCR_TOKEN` ou `DOCKER_USERNAME`/`DOCKER_PASSWORD` existent
3. R√©g√©n√©rer le token si n√©cessaire:
   - GitHub: Settings ‚Üí Developer settings ‚Üí Personal access tokens
   - Permissions requises: `write:packages`, `read:packages`

---

#### Probl√®me: Image trop volumineuse

**Sympt√¥mes**:
- Build r√©ussit mais prend beaucoup de temps
- Image > 500MB
- Push lent vers le registry

**Solutions**:

```bash
# 1. V√©rifier la taille de l'image
docker images | grep portfolio

# 2. Analyser les layers
docker history portfolio:test

# 3. Utiliser .dockerignore
cat > .dockerignore << EOF
node_modules
.next
.git
.github
*.md
.env*
.vscode
.idea
coverage
EOF

# 4. Optimiser le Dockerfile
# - Utiliser Alpine images
# - Multi-stage build
# - Copier seulement les fichiers n√©cessaires
```

---

### √âchecs de D√©ploiement Kubernetes

#### Probl√®me: Les pods ne d√©marrent pas (ImagePullBackOff)

**Sympt√¥mes**:
- üî¥ Pods en √©tat `ImagePullBackOff` ou `ErrImagePull`
- Message: "Failed to pull image"

**Causes possibles**:
1. Image n'existe pas dans le registry
2. Tag d'image incorrect
3. Credentials de registry manquants
4. Registry inaccessible depuis le cluster

**Solutions**:

```bash
# 1. V√©rifier que l'image existe
docker pull ghcr.io/username/portfolio:tag

# 2. V√©rifier le tag dans le deployment
kubectl get deployment portfolio -o yaml | grep image:

# 3. V√©rifier les secrets de registry
kubectl get secrets
kubectl describe secret regcred

# 4. Cr√©er un secret de registry si n√©cessaire
kubectl create secret docker-registry regcred \
  --docker-server=ghcr.io \
  --docker-username=USERNAME \
  --docker-password=TOKEN \
  --docker-email=EMAIL

# 5. Ajouter le secret au deployment
kubectl patch deployment portfolio -p '{"spec":{"template":{"spec":{"imagePullSecrets":[{"name":"regcred"}]}}}}'
```

**V√©rification dans Lens**:
1. Workloads ‚Üí Pods
2. Cliquer sur le pod en erreur
3. Onglet "Events" ‚Üí Voir les messages d'erreur d√©taill√©s
4. V√©rifier la section "Image Pull" dans les √©v√©nements

---

#### Probl√®me: Pods en CrashLoopBackOff

**Sympt√¥mes**:
- üî¥ Pods red√©marrent continuellement
- √âtat: `CrashLoopBackOff`
- Restart count augmente constamment

**Causes possibles**:
1. Application crash au d√©marrage
2. Port d√©j√† utilis√©
3. Variables d'environnement manquantes
4. Erreurs de configuration

**Solutions**:

```bash
# 1. Voir les logs du pod
kubectl logs <pod-name>
kubectl logs <pod-name> --previous  # Logs du container pr√©c√©dent

# 2. Voir les √©v√©nements
kubectl describe pod <pod-name>

# 3. Ex√©cuter un shell dans le pod (si possible)
kubectl exec -it <pod-name> -- /bin/sh

# 4. V√©rifier les variables d'environnement
kubectl exec <pod-name> -- env

# 5. Tester l'application localement avec la m√™me config
docker run -it portfolio:test /bin/sh
node server.js
```

**V√©rification dans Lens**:
1. Workloads ‚Üí Pods
2. Cliquer sur le pod crash√©
3. Onglet "Logs" ‚Üí Voir les erreurs
4. Onglet "Events" ‚Üí Voir l'historique des crashes
5. V√©rifier les "Container Status" pour voir la raison du crash

**Causes communes et solutions**:

| Erreur dans les logs | Solution |
|---------------------|----------|
| `Error: Cannot find module` | V√©rifier que tous les fichiers sont copi√©s dans l'image |
| `EADDRINUSE: address already in use` | V√©rifier le port dans le Dockerfile (doit √™tre 3000) |
| `Error: Missing environment variable` | Ajouter la variable dans ConfigMap ou Secret |
| `Permission denied` | V√©rifier les permissions des fichiers dans l'image |

---

#### Probl√®me: Pods en √©tat Pending

**Sympt√¥mes**:
- üü° Pods restent en √©tat `Pending`
- Ne passent jamais √† `Running`

**Causes possibles**:
1. Ressources insuffisantes dans le cluster
2. Aucun node disponible
3. PersistentVolume non disponible
4. Contraintes de scheduling non satisfaites

**Solutions**:

```bash
# 1. Voir pourquoi le pod est pending
kubectl describe pod <pod-name>

# 2. V√©rifier les ressources du cluster
kubectl top nodes
kubectl describe nodes

# 3. V√©rifier les events
kubectl get events --sort-by='.lastTimestamp' | grep <pod-name>

# 4. R√©duire les resource requests si n√©cessaire
kubectl edit deployment portfolio
# Modifier requests.memory et requests.cpu
```

**V√©rification dans Lens**:
1. Workloads ‚Üí Pods
2. Cliquer sur le pod Pending
3. Onglet "Events" ‚Üí Chercher "FailedScheduling"
4. Cluster ‚Üí Nodes ‚Üí V√©rifier les ressources disponibles

**Messages courants**:

| Message | Solution |
|---------|----------|
| `Insufficient cpu` | R√©duire les CPU requests ou ajouter des nodes |
| `Insufficient memory` | R√©duire les memory requests ou ajouter des nodes |
| `No nodes available` | V√©rifier que le cluster a des nodes actifs |
| `PersistentVolumeClaim not found` | Cr√©er le PVC ou retirer la d√©pendance |

---

#### Probl√®me: Deployment bloqu√© (Progressing)

**Sympt√¥mes**:
- ‚è≥ Deployment reste en √©tat "Progressing"
- Rollout ne se termine jamais
- Anciens pods restent actifs

**Causes possibles**:
1. Nouveaux pods ne passent pas les health checks
2. Strat√©gie de rolling update bloqu√©e
3. Ressources insuffisantes

**Solutions**:

```bash
# 1. Voir le status du rollout
kubectl rollout status deployment/portfolio

# 2. Voir l'historique
kubectl rollout history deployment/portfolio

# 3. Voir les d√©tails du deployment
kubectl describe deployment portfolio

# 4. Annuler le rollout en cours
kubectl rollout undo deployment/portfolio

# 5. Forcer un nouveau rollout
kubectl rollout restart deployment/portfolio
```

**V√©rification dans Lens**:
1. Workloads ‚Üí Deployments ‚Üí portfolio
2. Voir "Conditions" ‚Üí Chercher "Progressing"
3. V√©rifier "Replicas": devrait √™tre "3/3"
4. Onglet "Pods" ‚Üí V√©rifier l'√©tat de chaque pod

---

### Probl√®mes de Health Check

#### Probl√®me: Readiness probe √©choue

**Sympt√¥mes**:
- üü° Pods en √©tat `Running` mais pas `Ready`
- Indicateur: `0/1` dans la colonne READY
- Pas de trafic rout√© vers les pods

**Causes possibles**:
1. Endpoint `/api/health` ne r√©pond pas
2. Application d√©marre lentement
3. Port incorrect dans la probe
4. D√©lai initial trop court

**Solutions**:

```bash
# 1. Tester l'endpoint depuis le pod
kubectl exec <pod-name> -- wget -O- http://localhost:3000/api/health

# 2. Voir les logs de l'application
kubectl logs <pod-name>

# 3. V√©rifier la configuration de la probe
kubectl get deployment portfolio -o yaml | grep -A 10 readinessProbe

# 4. Augmenter le d√©lai initial si n√©cessaire
kubectl patch deployment portfolio -p '
{
  "spec": {
    "template": {
      "spec": {
        "containers": [{
          "name": "portfolio",
          "readinessProbe": {
            "initialDelaySeconds": 15
          }
        }]
      }
    }
  }
}'
```

**Configuration recommand√©e**:

```yaml
readinessProbe:
  httpGet:
    path: /api/health
    port: 3000
  initialDelaySeconds: 5   # Attendre 5s apr√®s le d√©marrage
  periodSeconds: 5         # V√©rifier toutes les 5s
  timeoutSeconds: 3        # Timeout apr√®s 3s
  successThreshold: 1      # 1 succ√®s = ready
  failureThreshold: 3      # 3 √©checs = not ready
```

**V√©rification dans Lens**:
1. Workloads ‚Üí Pods
2. Cliquer sur le pod
3. Section "Conditions" ‚Üí V√©rifier "Ready: False"
4. Onglet "Events" ‚Üí Chercher "Readiness probe failed"

---

#### Probl√®me: Liveness probe √©choue (pods red√©marrent)

**Sympt√¥mes**:
- üî¥ Pods red√©marrent fr√©quemment
- Restart count augmente
- Message: "Liveness probe failed"

**Causes possibles**:
1. Application devient non-responsive
2. Probe trop agressive
3. Ressources insuffisantes (CPU throttling)
4. Deadlock dans l'application

**Solutions**:

```bash
# 1. Voir les logs avant le restart
kubectl logs <pod-name> --previous

# 2. V√©rifier la configuration de la liveness probe
kubectl get deployment portfolio -o yaml | grep -A 10 livenessProbe

# 3. Augmenter les seuils de tol√©rance
kubectl patch deployment portfolio -p '
{
  "spec": {
    "template": {
      "spec": {
        "containers": [{
          "name": "portfolio",
          "livenessProbe": {
            "initialDelaySeconds": 30,
            "periodSeconds": 10,
            "timeoutSeconds": 5,
            "failureThreshold": 5
          }
        }]
      }
    }
  }
}'

# 4. V√©rifier l'utilisation des ressources
kubectl top pod <pod-name>
```

**Configuration recommand√©e**:

```yaml
livenessProbe:
  httpGet:
    path: /api/health
    port: 3000
  initialDelaySeconds: 30  # Attendre 30s (temps de d√©marrage)
  periodSeconds: 10        # V√©rifier toutes les 10s
  timeoutSeconds: 5        # Timeout apr√®s 5s
  failureThreshold: 3      # 3 √©checs cons√©cutifs = restart
```

**V√©rification dans Lens**:
1. Workloads ‚Üí Pods
2. Voir la colonne "Restarts" (devrait √™tre 0 ou faible)
3. Cliquer sur un pod avec restarts √©lev√©s
4. Onglet "Events" ‚Üí Chercher "Liveness probe failed"
5. Onglet "Logs" ‚Üí S√©lectionner "Previous" pour voir les logs avant restart

---

### Probl√®mes de Certificats TLS

#### Probl√®me: Certificat non provisionn√©

**Sympt√¥mes**:
- ‚ö†Ô∏è HTTPS ne fonctionne pas
- Erreur: "Certificate not found"
- Ingress sans TLS

**Causes possibles**:
1. cert-manager non install√©
2. ClusterIssuer non configur√©
3. DNS non configur√© correctement
4. Challenge Let's Encrypt √©choue

**Solutions**:

```bash
# 1. V√©rifier que cert-manager est install√©
kubectl get pods -n cert-manager

# 2. V√©rifier le ClusterIssuer
kubectl get clusterissuer
kubectl describe clusterissuer letsencrypt-prod

# 3. V√©rifier le Certificate
kubectl get certificate portfolio-tls
kubectl describe certificate portfolio-tls

# 4. Voir les challenges en cours
kubectl get challenges
kubectl describe challenge <challenge-name>

# 5. V√©rifier les logs de cert-manager
kubectl logs -n cert-manager deployment/cert-manager
```

**Installer cert-manager si n√©cessaire**:

```bash
# Installation via kubectl
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml

# Cr√©er un ClusterIssuer
cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: votre-email@example.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx
EOF
```

**V√©rification dans Lens**:
1. Config ‚Üí Certificates ‚Üí V√©rifier "portfolio-tls"
2. Status devrait √™tre "Ready: True"
3. Si "Ready: False", voir les "Events" pour les erreurs
4. Network ‚Üí Ingresses ‚Üí V√©rifier la section TLS

---

#### Probl√®me: Certificat expir√© ou invalide

**Sympt√¥mes**:
- ‚ö†Ô∏è Navigateur affiche "Certificate expired"
- HTTPS fonctionne mais avec avertissement

**Causes possibles**:
1. Renouvellement automatique √©chou√©
2. Certificat de staging utilis√©
3. DNS chang√©

**Solutions**:

```bash
# 1. V√©rifier l'expiration du certificat
kubectl get certificate portfolio-tls -o yaml

# 2. Forcer le renouvellement
kubectl delete certificate portfolio-tls
# Le certificat sera recr√©√© automatiquement

# 3. V√©rifier que vous utilisez le bon issuer
kubectl get ingress portfolio-ingress -o yaml | grep issuer
# Devrait √™tre: letsencrypt-prod (pas letsencrypt-staging)

# 4. Tester le certificat
openssl s_client -connect votre-domaine.com:443 -servername votre-domaine.com
```

**V√©rification dans Lens**:
1. Config ‚Üí Certificates ‚Üí portfolio-tls
2. V√©rifier "Not After" (date d'expiration)
3. V√©rifier "Renewal Time" (date de renouvellement)

---

#### Probl√®me: Redirection HTTP ‚Üí HTTPS ne fonctionne pas

**Sympt√¥mes**:
- HTTP fonctionne mais pas de redirection vers HTTPS
- Acc√®s en http:// reste en HTTP

**Solution**:

```bash
# V√©rifier l'annotation dans l'Ingress
kubectl get ingress portfolio-ingress -o yaml | grep ssl-redirect

# Ajouter l'annotation si manquante
kubectl annotate ingress portfolio-ingress nginx.ingress.kubernetes.io/ssl-redirect="true"
```

**Configuration correcte de l'Ingress**:

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: portfolio-ingress
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - votre-domaine.com
    secretName: portfolio-tls
  rules:
  - host: votre-domaine.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: portfolio-service
            port:
              number: 80
```

---

### Probl√®mes de Secrets

#### Probl√®me: Secrets non trouv√©s dans le cluster

**Sympt√¥mes**:
- Pods ne d√©marrent pas
- Erreur: "Secret not found"
- Variables d'environnement manquantes

**Solutions**:

```bash
# 1. Lister les secrets
kubectl get secrets

# 2. V√©rifier un secret sp√©cifique
kubectl describe secret <secret-name>

# 3. Cr√©er un secret depuis un fichier
kubectl create secret generic portfolio-secrets \
  --from-literal=API_KEY=votre-cle \
  --from-literal=DATABASE_URL=votre-url

# 4. Cr√©er depuis un fichier .env
kubectl create secret generic portfolio-secrets --from-env-file=.env

# 5. V√©rifier que le deployment r√©f√©rence le bon secret
kubectl get deployment portfolio -o yaml | grep -A 5 secretRef
```

**V√©rification dans Lens**:
1. Config ‚Üí Secrets
2. V√©rifier que le secret existe
3. Cliquer dessus pour voir les cl√©s (valeurs masqu√©es)
4. Workloads ‚Üí Deployments ‚Üí portfolio
5. V√©rifier la section "Environment Variables"

---

#### Probl√®me: Secrets expos√©s dans les logs GitHub Actions

**Sympt√¥mes**:
- Valeurs sensibles visibles dans les logs
- Avertissement de s√©curit√© GitHub

**Solutions**:

```bash
# Les secrets GitHub sont automatiquement masqu√©s
# Si un secret appara√Æt, c'est qu'il n'est pas stock√© comme secret

# 1. V√©rifier que la valeur est bien un secret GitHub
# GitHub ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions

# 2. Utiliser les secrets correctement dans le workflow
# ‚úÖ Correct:
echo "${{ secrets.DOCKER_PASSWORD }}" | docker login ...

# ‚ùå Incorrect (expose le secret):
echo "Password: ${{ secrets.DOCKER_PASSWORD }}"

# 3. R√©voquer et r√©g√©n√©rer tout secret expos√©
```

**Bonnes pratiques**:
- Ne jamais echo ou print les secrets
- Utiliser `::add-mask::` pour masquer des valeurs dynamiques
- R√©voquer imm√©diatement tout secret expos√©

---

## Commandes de Debugging

### Commandes Kubernetes Essentielles

```bash
# ============================================
# PODS
# ============================================

# Lister tous les pods
kubectl get pods

# Lister avec plus de d√©tails
kubectl get pods -o wide

# Voir les pods de tous les namespaces
kubectl get pods --all-namespaces

# D√©crire un pod (√©v√©nements, status, etc.)
kubectl describe pod <pod-name>

# Voir les logs d'un pod
kubectl logs <pod-name>

# Suivre les logs en temps r√©el
kubectl logs -f <pod-name>

# Voir les logs du container pr√©c√©dent (apr√®s crash)
kubectl logs <pod-name> --previous

# Logs de tous les pods d'un deployment
kubectl logs -l app=portfolio --tail=50

# Ex√©cuter une commande dans un pod
kubectl exec <pod-name> -- ls -la

# Ouvrir un shell interactif
kubectl exec -it <pod-name> -- /bin/sh

# Copier des fichiers depuis/vers un pod
kubectl cp <pod-name>:/path/to/file ./local-file
kubectl cp ./local-file <pod-name>:/path/to/file

# ============================================
# DEPLOYMENTS
# ============================================

# Lister les deployments
kubectl get deployments

# D√©crire un deployment
kubectl describe deployment portfolio

# Voir le status du rollout
kubectl rollout status deployment/portfolio

# Voir l'historique des rollouts
kubectl rollout history deployment/portfolio

# Voir les d√©tails d'une r√©vision
kubectl rollout history deployment/portfolio --revision=2

# Mettre √† l'√©chelle un deployment
kubectl scale deployment portfolio --replicas=5

# √âditer un deployment
kubectl edit deployment portfolio

# Red√©marrer un deployment (rolling restart)
kubectl rollout restart deployment/portfolio

# ============================================
# SERVICES
# ============================================

# Lister les services
kubectl get services

# D√©crire un service
kubectl describe service portfolio-service

# Voir les endpoints (IPs des pods)
kubectl get endpoints portfolio-service

# Port-forward pour acc√®s local
kubectl port-forward service/portfolio-service 8080:80

# ============================================
# INGRESS
# ============================================

# Lister les ingress
kubectl get ingress

# D√©crire un ingress
kubectl describe ingress portfolio-ingress

# Voir l'IP externe
kubectl get ingress portfolio-ingress -o jsonpath='{.status.loadBalancer.ingress[0].ip}'

# ============================================
# CONFIGMAPS & SECRETS
# ============================================

# Lister les ConfigMaps
kubectl get configmaps

# Voir le contenu d'un ConfigMap
kubectl get configmap <name> -o yaml

# Lister les Secrets
kubectl get secrets

# Voir les cl√©s d'un Secret (valeurs encod√©es)
kubectl get secret <name> -o yaml

# D√©coder une valeur de secret
kubectl get secret <name> -o jsonpath='{.data.key}' | base64 -d

# ============================================
# √âV√âNEMENTS
# ============================================

# Voir tous les √©v√©nements r√©cents
kubectl get events --sort-by='.lastTimestamp'

# √âv√©nements d'un namespace sp√©cifique
kubectl get events -n default --sort-by='.lastTimestamp'

# Filtrer les √©v√©nements par type
kubectl get events --field-selector type=Warning

# ============================================
# RESSOURCES & PERFORMANCE
# ============================================

# Voir l'utilisation des ressources des nodes
kubectl top nodes

# Voir l'utilisation des ressources des pods
kubectl top pods

# Voir l'utilisation d'un pod sp√©cifique
kubectl top pod <pod-name>

# ============================================
# DEBUGGING AVANC√â
# ============================================

# Cr√©er un pod de debug temporaire
kubectl run debug --image=alpine --rm -it --restart=Never -- sh

# Tester la connectivit√© r√©seau
kubectl run debug --image=nicolaka/netshoot --rm -it --restart=Never -- sh

# Dans le pod de debug:
# curl http://portfolio-service/api/health
# nslookup portfolio-service
# ping portfolio-service

# V√©rifier les DNS
kubectl run -it --rm debug --image=busybox --restart=Never -- nslookup portfolio-service

# ============================================
# NETTOYAGE
# ============================================

# Supprimer un pod (sera recr√©√© par le deployment)
kubectl delete pod <pod-name>

# Supprimer un deployment
kubectl delete deployment portfolio

# Supprimer toutes les ressources d'un label
kubectl delete all -l app=portfolio

# Forcer la suppression d'un pod bloqu√©
kubectl delete pod <pod-name> --force --grace-period=0
```

### Commandes Docker pour Tests Locaux

```bash
# ============================================
# BUILD & RUN
# ============================================

# Build de l'image
docker build -t portfolio:test .

# Build sans cache
docker build --no-cache -t portfolio:test .

# Build avec logs d√©taill√©s
docker build --progress=plain -t portfolio:test .

# Lancer le container
docker run -p 3000:3000 portfolio:test

# Lancer en mode d√©tach√©
docker run -d -p 3000:3000 --name portfolio-test portfolio:test

# Lancer avec variables d'environnement
docker run -p 3000:3000 -e NODE_ENV=production portfolio:test

# ============================================
# DEBUGGING
# ============================================

# Voir les containers en cours
docker ps

# Voir tous les containers (y compris arr√™t√©s)
docker ps -a

# Voir les logs d'un container
docker logs portfolio-test

# Suivre les logs
docker logs -f portfolio-test

# Ex√©cuter une commande dans un container
docker exec portfolio-test ls -la

# Ouvrir un shell dans un container
docker exec -it portfolio-test /bin/sh

# Inspecter un container
docker inspect portfolio-test

# Voir les stats en temps r√©el
docker stats portfolio-test

# ============================================
# IMAGES
# ============================================

# Lister les images
docker images

# Voir l'historique des layers
docker history portfolio:test

# Analyser la taille des layers
docker history portfolio:test --no-trunc

# Supprimer une image
docker rmi portfolio:test

# ============================================
# NETTOYAGE
# ============================================

# Arr√™ter un container
docker stop portfolio-test

# Supprimer un container
docker rm portfolio-test

# Arr√™ter et supprimer
docker rm -f portfolio-test

# Nettoyer les images inutilis√©es
docker image prune

# Nettoyer tout (images, containers, volumes, networks)
docker system prune -a

# Voir l'espace utilis√©
docker system df
```

### Commandes GitHub Actions

```bash
# ============================================
# GITHUB CLI (gh)
# ============================================

# Installer GitHub CLI
# macOS: brew install gh
# Linux: voir https://cli.github.com/

# Se connecter
gh auth login

# Lister les workflows
gh workflow list

# Voir les runs d'un workflow
gh run list --workflow=deploy.yml

# Voir les d√©tails d'un run
gh run view <run-id>

# Voir les logs d'un run
gh run view <run-id> --log

# Relancer un workflow √©chou√©
gh run rerun <run-id>

# D√©clencher un workflow manuellement
gh workflow run deploy.yml

# Voir le status du dernier run
gh run list --limit 1
```

---

## Proc√©dures de D√©ploiement Manuel

### D√©ploiement Manuel Complet

Si le pipeline CI/CD ne fonctionne pas, voici comment d√©ployer manuellement:

```bash
# ============================================
# √âTAPE 1: BUILD LOCAL
# ============================================

# 1. Cloner le repository
git clone https://github.com/username/monportfolio.git
cd monportfolio

# 2. Installer les d√©pendances
npm install

# 3. Tester localement
npm run build
npm test

# ============================================
# √âTAPE 2: BUILD DOCKER
# ============================================

# 4. Build de l'image Docker
docker build -t portfolio:manual-$(date +%Y%m%d-%H%M%S) .

# 5. Tagger l'image
docker tag portfolio:manual-$(date +%Y%m%d-%H%M%S) ghcr.io/username/portfolio:manual
docker tag portfolio:manual-$(date +%Y%m%d-%H%M%S) ghcr.io/username/portfolio:latest

# 6. Se connecter au registry
echo $GHCR_TOKEN | docker login ghcr.io -u username --password-stdin

# 7. Pusher l'image
docker push ghcr.io/username/portfolio:manual
docker push ghcr.io/username/portfolio:latest

# ============================================
# √âTAPE 3: D√âPLOIEMENT KUBERNETES
# ============================================

# 8. Se connecter au cluster
export KUBECONFIG=~/.kube/config
kubectl cluster-info

# 9. Mettre √† jour le tag dans le deployment
kubectl set image deployment/portfolio portfolio=ghcr.io/username/portfolio:manual

# Ou √©diter directement:
kubectl edit deployment portfolio
# Changer .spec.template.spec.containers[0].image

# 10. V√©rifier le rollout
kubectl rollout status deployment/portfolio

# 11. V√©rifier que les pods sont Running
kubectl get pods -l app=portfolio

# 12. Tester l'application
kubectl port-forward service/portfolio-service 8080:80
curl http://localhost:8080/api/health

# ============================================
# √âTAPE 4: V√âRIFICATION
# ============================================

# 13. V√©rifier les logs
kubectl logs -l app=portfolio --tail=50

# 14. V√©rifier l'Ingress
kubectl get ingress portfolio-ingress

# 15. Tester en production
curl https://votre-domaine.com/api/health
```

### D√©ploiement d'une Branche Sp√©cifique

```bash
# 1. Checkout de la branche
git checkout feature-branch

# 2. Build avec un tag sp√©cifique
docker build -t ghcr.io/username/portfolio:feature-branch .

# 3. Push
docker push ghcr.io/username/portfolio:feature-branch

# 4. D√©ployer dans un namespace s√©par√© (staging)
kubectl create namespace staging

# 5. Copier les secrets dans le namespace staging
kubectl get secret portfolio-secrets -o yaml | \
  sed 's/namespace: default/namespace: staging/' | \
  kubectl apply -f -

# 6. D√©ployer avec le nouveau tag
kubectl apply -f k8s/ -n staging
kubectl set image deployment/portfolio portfolio=ghcr.io/username/portfolio:feature-branch -n staging

# 7. V√©rifier
kubectl get pods -n staging
```

### Mise √† Jour des Manifests Kubernetes

```bash
# Appliquer tous les manifests
kubectl apply -f k8s/

# Appliquer un manifest sp√©cifique
kubectl apply -f k8s/deployment.yaml

# Dry-run pour v√©rifier sans appliquer
kubectl apply -f k8s/ --dry-run=client

# Voir les diff√©rences avant d'appliquer
kubectl diff -f k8s/

# Forcer le remplacement
kubectl replace --force -f k8s/deployment.yaml
```

---

## Proc√©dures de Rollback

### Rollback Automatique

Kubernetes effectue un rollback automatique si:
- Les nouveaux pods ne passent pas les health checks
- Le deployment ne progresse pas apr√®s `progressDeadlineSeconds` (600s par d√©faut)

**V√©rification**:
```bash
# Voir si un rollback automatique a eu lieu
kubectl describe deployment portfolio | grep -A 10 Conditions
```

### Rollback Manuel via kubectl

```bash
# ============================================
# M√âTHODE 1: Rollback √† la r√©vision pr√©c√©dente
# ============================================

# Annuler le dernier d√©ploiement
kubectl rollout undo deployment/portfolio

# V√©rifier le status
kubectl rollout status deployment/portfolio

# ============================================
# M√âTHODE 2: Rollback √† une r√©vision sp√©cifique
# ============================================

# 1. Voir l'historique des d√©ploiements
kubectl rollout history deployment/portfolio

# Sortie exemple:
# REVISION  CHANGE-CAUSE
# 1         <none>
# 2         Image updated to sha-abc123
# 3         Image updated to sha-def456

# 2. Voir les d√©tails d'une r√©vision
kubectl rollout history deployment/portfolio --revision=2

# 3. Rollback √† cette r√©vision
kubectl rollout undo deployment/portfolio --to-revision=2

# 4. V√©rifier
kubectl rollout status deployment/portfolio
kubectl get pods -l app=portfolio

# ============================================
# M√âTHODE 3: Rollback via image tag
# ============================================

# 1. Lister les images disponibles dans le registry
# (via l'interface web du registry)

# 2. D√©ployer une version sp√©cifique
kubectl set image deployment/portfolio \
  portfolio=ghcr.io/username/portfolio:sha-abc123

# 3. V√©rifier
kubectl rollout status deployment/portfolio
```

### Rollback via Lens

**Proc√©dure visuelle**:

1. **Ouvrir Lens**
2. **Naviguer vers Workloads ‚Üí Deployments**
3. **Cliquer sur `portfolio`**
4. **Onglet "Revisions"**
   - Voir toutes les r√©visions avec timestamps
   - Voir les changements de chaque r√©vision
5. **S√©lectionner une r√©vision pr√©c√©dente**
6. **Cliquer sur "Rollback to this revision"**
7. **Confirmer**
8. **V√©rifier dans l'onglet "Pods"** que les nouveaux pods d√©marrent

**Avantages de Lens**:
- Interface visuelle claire
- Voir les diff√©rences entre r√©visions
- Rollback en un clic
- Monitoring en temps r√©el du rollback

### Rollback via GitHub Actions Workflow

Si vous avez cr√©√© le workflow de rollback (task 5):

```bash
# 1. Aller sur GitHub ‚Üí Actions
# 2. S√©lectionner le workflow "Rollback"
# 3. Cliquer sur "Run workflow"
# 4. Entrer le tag de l'image √† d√©ployer (ex: sha-abc123)
# 5. Cliquer sur "Run workflow"
# 6. Suivre l'ex√©cution
```

**Ou via GitHub CLI**:

```bash
# D√©clencher le workflow de rollback
gh workflow run rollback.yml -f image_tag=sha-abc123

# Suivre l'ex√©cution
gh run watch
```

### Rollback d'Urgence (Downtime Acceptable)

Si vous devez rollback imm√©diatement et acceptez un court downtime:

```bash
# 1. Supprimer le deployment actuel
kubectl delete deployment portfolio

# 2. R√©appliquer avec l'ancienne version
# √âditer k8s/deployment.yaml pour changer l'image tag
nano k8s/deployment.yaml

# 3. R√©appliquer
kubectl apply -f k8s/deployment.yaml

# 4. V√©rifier
kubectl get pods -l app=portfolio
```

### V√©rification Apr√®s Rollback

```bash
# 1. V√©rifier que tous les pods sont Running et Ready
kubectl get pods -l app=portfolio

# Sortie attendue:
# NAME                         READY   STATUS    RESTARTS   AGE
# portfolio-xxx                1/1     Running   0          2m
# portfolio-yyy                1/1     Running   0          2m
# portfolio-zzz                1/1     Running   0          2m

# 2. V√©rifier les logs
kubectl logs -l app=portfolio --tail=20

# 3. Tester l'endpoint de health
kubectl port-forward service/portfolio-service 8080:80
curl http://localhost:8080/api/health

# 4. Tester en production
curl https://votre-domaine.com/api/health

# 5. V√©rifier dans Lens
# - Tous les pods verts
# - Pas d'√©v√©nements d'erreur
# - M√©triques normales
```

### Pr√©vention des Probl√®mes de Rollback

**Bonnes pratiques**:

1. **Toujours tester avant de d√©ployer**
   ```bash
   npm test
   npm run build
   docker build -t test .
   docker run -p 3000:3000 test
   ```

2. **Utiliser des tags d'image explicites**
   - ‚úÖ `sha-abc123` (tra√ßable)
   - ‚ùå `latest` (ambigu)

3. **Garder un historique des d√©ploiements**
   ```bash
   # Annoter les d√©ploiements
   kubectl annotate deployment portfolio \
     kubernetes.io/change-cause="Deploy version 1.2.3 - Fix bug #123"
   ```

4. **Tester les health checks**
   - S'assurer que `/api/health` fonctionne toujours
   - Tester localement avant de d√©ployer

5. **Monitorer apr√®s chaque d√©ploiement**
   - V√©rifier les logs pendant 5-10 minutes
   - V√©rifier les m√©triques dans Lens
   - Tester les fonctionnalit√©s critiques

---

## Utilisation de Lens pour le Debugging

### Installation et Configuration

```bash
# 1. T√©l√©charger Lens depuis https://k8slens.dev
# 2. Installer l'application
# 3. Lancer Lens
# 4. Lens d√©tecte automatiquement ~/.kube/config
```

### Navigation dans Lens

#### Vue d'Ensemble du Cluster

1. **Cluster ‚Üí Overview**
   - Voir le nombre de nodes, pods, services
   - Voir l'utilisation globale des ressources
   - Voir les √©v√©nements r√©cents

2. **Cluster ‚Üí Nodes**
   - Liste de tous les nodes
   - Utilisation CPU/RAM par node
   - Pods par node
   - Cliquer sur un node pour voir les d√©tails

#### Debugging des Pods

**√âtapes pour debugger un pod**:

1. **Workloads ‚Üí Pods**
2. **Filtrer par label**: `app=portfolio`
3. **Identifier le pod probl√©matique** (ic√¥ne rouge/jaune)
4. **Cliquer sur le pod**

**Onglets disponibles**:

- **Overview**
  - Status du pod (Running, Pending, CrashLoopBackOff, etc.)
  - Node sur lequel il tourne
  - IP du pod
  - QoS Class
  - Conditions (Ready, Initialized, etc.)

- **Logs**
  - Logs en temps r√©el
  - Options:
    - Follow (suivre en temps r√©el)
    - Wrap (retour √† la ligne)
    - Timestamps
    - Previous (logs du container pr√©c√©dent)
  - Recherche dans les logs (Ctrl+F)
  - T√©l√©charger les logs

- **Shell**
  - Terminal interactif dans le pod
  - Commandes utiles:
    ```bash
    # Voir les fichiers
    ls -la
    
    # Tester l'application
    wget -O- http://localhost:3000/api/health
    
    # Voir les variables d'environnement
    env
    
    # Voir les processus
    ps aux
    
    # Tester la connectivit√©
    ping google.com
    ```

- **Events**
  - Historique des √©v√©nements du pod
  - Erreurs de scheduling
  - Erreurs de pull d'image
  - √âchecs de health checks
  - Restarts

#### Debugging des Deployments

1. **Workloads ‚Üí Deployments**
2. **Cliquer sur `portfolio`**

**Informations disponibles**:

- **Overview**
  - Nombre de replicas (desired vs actual)
  - Strat√©gie de mise √† jour
  - Conditions (Available, Progressing)
  - S√©lecteurs et labels

- **Pods**
  - Liste des pods du deployment
  - Status de chaque pod
  - Acc√®s rapide aux logs

- **Events**
  - √âv√©nements du deployment
  - √âchecs de scaling
  - Probl√®mes de rollout

- **Revisions**
  - Historique des r√©visions
  - Changements entre r√©visions
  - Rollback en un clic

#### Debugging des Services et Ingress

**Services**:

1. **Network ‚Üí Services**
2. **Cliquer sur `portfolio-service`**
3. **Voir**:
   - Type de service (ClusterIP, NodePort, LoadBalancer)
   - Ports expos√©s
   - Endpoints (IPs des pods)
   - S√©lecteurs

**Ingress**:

1. **Network ‚Üí Ingresses**
2. **Cliquer sur `portfolio-ingress`**
3. **Voir**:
   - R√®gles de routage
   - Configuration TLS
   - Backend services
   - Annotations

#### Debugging des ConfigMaps et Secrets

**ConfigMaps**:

1. **Config ‚Üí ConfigMaps**
2. **Cliquer sur le ConfigMap**
3. **Voir les donn√©es** (en clair)
4. **√âditer directement** si n√©cessaire

**Secrets**:

1. **Config ‚Üí Secrets**
2. **Cliquer sur le Secret**
3. **Voir les cl√©s** (valeurs masqu√©es par d√©faut)
4. **R√©v√©ler les valeurs** (ic√¥ne ≈ìil)
5. **√âditer** si n√©cessaire

#### Monitoring des Ressources

**M√©triques en temps r√©el**:

1. **Workloads ‚Üí Pods**
2. **Voir les colonnes**:
   - CPU usage (%)
   - Memory usage (MB)
   - Restarts
   - Age

3. **Cliquer sur un pod ‚Üí Metrics**
   - Graphiques CPU over time
   - Graphiques Memory over time
   - Network I/O

**Alertes visuelles**:

- üü¢ **Vert**: Tout va bien
- üü° **Jaune**: Avertissement (ressources √©lev√©es, restarts)
- üî¥ **Rouge**: Erreur (pod crash√©, health check failed)

#### Fonctionnalit√©s Avanc√©es

**Port Forwarding**:

1. Cliquer sur un pod ou service
2. Cliquer sur l'ic√¥ne "Port Forward"
3. Choisir le port local et le port du pod
4. Acc√©der √† `localhost:<port>` dans le navigateur

**Ex√©cution de Commandes**:

1. Workloads ‚Üí Pods ‚Üí Cliquer sur un pod
2. Onglet "Shell"
3. Terminal interactif complet

**√âdition de Ressources**:

1. Cliquer sur n'importe quelle ressource
2. Bouton "Edit" (en haut √† droite)
3. √âditer le YAML directement
4. Sauvegarder (applique les changements)

**Suppression de Ressources**:

1. Cliquer sur une ressource
2. Bouton "Delete" (en haut √† droite)
3. Confirmer

### Sc√©narios de Debugging Courants avec Lens

#### Sc√©nario 1: Pod ne d√©marre pas

1. **Workloads ‚Üí Pods**
2. **Identifier le pod rouge**
3. **Cliquer dessus**
4. **Onglet "Events"** ‚Üí Voir la raison (ImagePullBackOff, CrashLoopBackOff, etc.)
5. **Onglet "Logs"** ‚Üí Voir les erreurs de l'application
6. **Corriger le probl√®me**
7. **Supprimer le pod** (sera recr√©√© automatiquement)

#### Sc√©nario 2: Application lente ou non-responsive

1. **Workloads ‚Üí Pods**
2. **V√©rifier la colonne CPU/Memory**
3. **Identifier les pods avec haute utilisation**
4. **Cliquer sur le pod ‚Üí Metrics**
5. **Voir les graphiques** pour identifier les pics
6. **Onglet "Shell"** ‚Üí Investiguer dans le pod
7. **Ajuster les resource limits** si n√©cessaire

#### Sc√©nario 3: Certificat TLS ne fonctionne pas

1. **Config ‚Üí Certificates**
2. **V√©rifier `portfolio-tls`**
3. **Status devrait √™tre "Ready: True"**
4. **Si "Ready: False"**:
   - Cliquer dessus
   - Onglet "Events" ‚Üí Voir les erreurs
   - V√©rifier les challenges Let's Encrypt
5. **Network ‚Üí Ingresses**
6. **V√©rifier la configuration TLS**

#### Sc√©nario 4: Rollback n√©cessaire

1. **Workloads ‚Üí Deployments ‚Üí portfolio**
2. **Onglet "Revisions"**
3. **Voir l'historique des d√©ploiements**
4. **S√©lectionner une r√©vision stable**
5. **Cliquer "Rollback to this revision"**
6. **Confirmer**
7. **Onglet "Pods"** ‚Üí V√©rifier que les nouveaux pods d√©marrent

---

## R√©cup√©ration apr√®s Incident

### Incident: Cluster Inaccessible

**Sympt√¥mes**:
- `kubectl` ne r√©pond pas
- Lens ne peut pas se connecter
- Erreur: "Unable to connect to the server"

**Actions**:

```bash
# 1. V√©rifier la connectivit√© r√©seau
ping <cluster-ip>

# 2. V√©rifier le kubeconfig
kubectl config view
kubectl config get-contexts

# 3. Tester la connexion
kubectl cluster-info

# 4. V√©rifier les credentials
kubectl config use-context <context-name>

# 5. Si le cluster est down, contacter l'administrateur
# ou red√©marrer les nodes si vous avez acc√®s
```

### Incident: Tous les Pods Crashent

**Sympt√¥mes**:
- Tous les pods en CrashLoopBackOff
- Application compl√®tement down

**Actions**:

```bash
# 1. Rollback imm√©diat
kubectl rollout undo deployment/portfolio

# 2. V√©rifier le status
kubectl rollout status deployment/portfolio

# 3. Si le rollback √©choue, d√©ployer une version connue
kubectl set image deployment/portfolio \
  portfolio=ghcr.io/username/portfolio:<version-stable>

# 4. Investiguer la cause
kubectl logs -l app=portfolio --previous --tail=100

# 5. Corriger et red√©ployer
```

### Incident: Certificat Expir√©

**Sympt√¥mes**:
- HTTPS ne fonctionne plus
- Erreur: "Certificate expired"

**Actions**:

```bash
# 1. V√©rifier l'expiration
kubectl get certificate portfolio-tls -o yaml

# 2. Forcer le renouvellement
kubectl delete certificate portfolio-tls

# 3. V√©rifier que cert-manager fonctionne
kubectl get pods -n cert-manager

# 4. Attendre la cr√©ation du nouveau certificat (2-5 minutes)
kubectl get certificate portfolio-tls --watch

# 5. V√©rifier que le certificat est Ready
kubectl describe certificate portfolio-tls
```

### Incident: Ressources √âpuis√©es

**Sympt√¥mes**:
- Pods en √©tat Pending
- Message: "Insufficient cpu" ou "Insufficient memory"

**Actions**:

```bash
# 1. V√©rifier les ressources du cluster
kubectl top nodes

# 2. Identifier les pods gourmands
kubectl top pods --all-namespaces --sort-by=memory
kubectl top pods --all-namespaces --sort-by=cpu

# 3. Option A: R√©duire les replicas temporairement
kubectl scale deployment portfolio --replicas=1

# 4. Option B: R√©duire les resource requests
kubectl edit deployment portfolio
# Modifier requests.memory et requests.cpu

# 5. Option C: Ajouter des nodes au cluster (si possible)

# 6. Nettoyer les ressources inutilis√©es
kubectl delete pods --field-selector=status.phase=Failed
kubectl delete pods --field-selector=status.phase=Succeeded
```

### Incident: Donn√©es Corrompues

**Sympt√¥mes**:
- Application d√©marre mais comportement anormal
- Erreurs de base de donn√©es

**Actions**:

```bash
# 1. Identifier le probl√®me
kubectl logs -l app=portfolio --tail=100

# 2. Si c'est un probl√®me de ConfigMap/Secret
kubectl get configmap portfolio-config -o yaml
kubectl get secret portfolio-secrets -o yaml

# 3. Corriger les donn√©es
kubectl edit configmap portfolio-config
kubectl edit secret portfolio-secrets

# 4. Red√©marrer les pods pour charger les nouvelles donn√©es
kubectl rollout restart deployment/portfolio

# 5. V√©rifier
kubectl logs -l app=portfolio --tail=50
```

### Plan de R√©cup√©ration d'Urgence

**Checklist de r√©cup√©ration**:

1. ‚úÖ **√âvaluer la situation**
   - Quel est le probl√®me exact?
   - Quelle est l'ampleur (1 pod, tous les pods, cluster entier)?
   - Y a-t-il des donn√©es perdues?

2. ‚úÖ **Stabiliser**
   - Rollback √† une version stable
   - R√©duire les replicas si n√©cessaire
   - Isoler les composants probl√©matiques

3. ‚úÖ **Communiquer**
   - Informer les utilisateurs (si applicable)
   - Documenter l'incident
   - Estimer le temps de r√©solution

4. ‚úÖ **Investiguer**
   - Collecter les logs
   - Identifier la cause racine
   - Documenter les findings

5. ‚úÖ **Corriger**
   - Appliquer le fix
   - Tester en staging (si disponible)
   - D√©ployer en production

6. ‚úÖ **V√©rifier**
   - Tous les pods Running?
   - Application accessible?
   - Fonctionnalit√©s OK?
   - M√©triques normales?

7. ‚úÖ **Post-mortem**
   - Documenter l'incident
   - Identifier les am√©liorations
   - Mettre √† jour les proc√©dures

---

## FAQ

### Questions G√©n√©rales

**Q: Combien de temps prend un d√©ploiement?**

R: En moyenne:
- Build et tests: 3-5 minutes
- Build Docker: 2-3 minutes
- Push vers registry: 1-2 minutes
- D√©ploiement Kubernetes: 2-3 minutes
- **Total: 8-13 minutes**

---

**Q: Puis-je d√©ployer sans passer par GitHub Actions?**

R: Oui, voir la section [Proc√©dures de D√©ploiement Manuel](#proc√©dures-de-d√©ploiement-manuel).

---

**Q: Comment voir les logs en temps r√©el?**

R: Plusieurs options:
```bash
# Via kubectl
kubectl logs -f -l app=portfolio

# Via Lens
# Workloads ‚Üí Pods ‚Üí Cliquer sur un pod ‚Üí Onglet "Logs" ‚Üí Activer "Follow"
```

---

**Q: Comment tester l'application avant qu'elle soit accessible publiquement?**

R: Utiliser port-forward:
```bash
kubectl port-forward service/portfolio-service 8080:80
# Puis acc√©der √† http://localhost:8080
```

---

### Questions sur les D√©ploiements

**Q: Pourquoi mon d√©ploiement prend-il autant de temps?**

R: Causes possibles:
- Pull de l'image Docker lent (image volumineuse ou connexion lente)
- Health checks avec d√©lai initial √©lev√©
- Ressources insuffisantes (pods en attente de ressources)

Solution: V√©rifier avec `kubectl describe deployment portfolio`

---

**Q: Puis-je d√©ployer plusieurs versions en m√™me temps?**

R: Oui, en utilisant des namespaces diff√©rents:
```bash
# Production
kubectl apply -f k8s/ -n production

# Staging
kubectl apply -f k8s/ -n staging
```

---

**Q: Comment annuler un d√©ploiement en cours?**

R:
```bash
# Rollback imm√©diat
kubectl rollout undo deployment/portfolio

# Ou pause puis undo
kubectl rollout pause deployment/portfolio
kubectl rollout undo deployment/portfolio
kubectl rollout resume deployment/portfolio
```

---

### Questions sur les Erreurs

**Q: Que signifie "ImagePullBackOff"?**

R: Kubernetes ne peut pas t√©l√©charger l'image Docker. Causes:
- Image n'existe pas
- Tag incorrect
- Credentials manquants
- Registry inaccessible

Voir: [Probl√®me: Les pods ne d√©marrent pas (ImagePullBackOff)](#probl√®me-les-pods-ne-d√©marrent-pas-imagepullbackoff)

---

**Q: Que signifie "CrashLoopBackOff"?**

R: Le pod d√©marre puis crash imm√©diatement, et Kubernetes le red√©marre en boucle. Causes:
- Erreur dans l'application
- Configuration incorrecte
- D√©pendances manquantes

Voir: [Probl√®me: Pods en CrashLoopBackOff](#probl√®me-pods-en-crashloopbackoff)

---

**Q: Pourquoi mes pods sont "Running" mais pas "Ready"?**

R: Les readiness probes √©chouent. L'application d√©marre mais ne r√©pond pas correctement au health check.

Voir: [Probl√®me: Readiness probe √©choue](#probl√®me-readiness-probe-√©choue)

---

### Questions sur Lens

**Q: Lens ne voit pas mon cluster**

R:
1. V√©rifier que `~/.kube/config` existe et est valide
2. Dans Lens: File ‚Üí Add Cluster ‚Üí Coller le kubeconfig
3. V√©rifier la connectivit√©: `kubectl cluster-info`

---

**Q: Comment voir les m√©triques dans Lens?**

R: Lens affiche automatiquement les m√©triques si metrics-server est install√© dans le cluster:
```bash
# V√©rifier metrics-server
kubectl get deployment metrics-server -n kube-system

# Installer si n√©cessaire
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

---

**Q: Puis-je √©diter les ressources directement dans Lens?**

R: Oui:
1. Cliquer sur n'importe quelle ressource
2. Bouton "Edit" en haut √† droite
3. Modifier le YAML
4. Sauvegarder (applique imm√©diatement)

---

### Questions sur la S√©curit√©

**Q: Mes secrets sont-ils s√©curis√©s?**

R: Oui, si vous suivez les bonnes pratiques:
- ‚úÖ Secrets stock√©s dans GitHub Secrets (chiffr√©s)
- ‚úÖ Secrets Kubernetes (chiffr√©s at rest)
- ‚úÖ Secrets masqu√©s dans les logs GitHub Actions
- ‚ùå Ne jamais commiter de secrets dans le code
- ‚ùå Ne jamais logger les secrets

---

**Q: Comment changer un secret?**

R:
```bash
# M√©thode 1: √âditer directement
kubectl edit secret portfolio-secrets

# M√©thode 2: Recr√©er
kubectl delete secret portfolio-secrets
kubectl create secret generic portfolio-secrets \
  --from-literal=KEY=nouvelle-valeur

# Red√©marrer les pods pour charger le nouveau secret
kubectl rollout restart deployment/portfolio
```

---

**Q: Comment savoir si mes secrets sont expos√©s?**

R: V√©rifier:
1. Logs GitHub Actions (secrets automatiquement masqu√©s)
2. Logs des pods: `kubectl logs <pod-name>` (ne devrait pas contenir de secrets)
3. Variables d'environnement: `kubectl exec <pod-name> -- env` (secrets visibles ici, c'est normal)

---

### Questions sur les Performances

**Q: Comment augmenter le nombre de replicas?**

R:
```bash
# Temporairement
kubectl scale deployment portfolio --replicas=5

# D√©finitivement
# √âditer k8s/deployment.yaml et changer replicas: 5
kubectl apply -f k8s/deployment.yaml
```

---

**Q: Comment activer l'autoscaling?**

R:
```bash
# Cr√©er un HorizontalPodAutoscaler
kubectl autoscale deployment portfolio \
  --cpu-percent=70 \
  --min=3 \
  --max=10

# V√©rifier
kubectl get hpa
```

---

**Q: Mon application est lente, comment investiguer?**

R:
1. V√©rifier les ressources:
   ```bash
   kubectl top pods -l app=portfolio
   ```
2. V√©rifier les logs pour des erreurs
3. Utiliser Lens pour voir les m√©triques en temps r√©el
4. Augmenter les resource limits si n√©cessaire

---

### Questions sur les Certificats

**Q: Combien de temps pour obtenir un certificat?**

R: G√©n√©ralement 2-5 minutes apr√®s la cr√©ation de l'Ingress. Let's Encrypt doit valider le domaine via HTTP-01 challenge.

---

**Q: Mon certificat est "Pending" depuis longtemps**

R: V√©rifier:
```bash
# Status du certificat
kubectl describe certificate portfolio-tls

# Challenges en cours
kubectl get challenges

# Logs de cert-manager
kubectl logs -n cert-manager deployment/cert-manager
```

Causes courantes:
- DNS pas encore propag√©
- Ingress Controller pas configur√© correctement
- Firewall bloque le port 80

---

**Q: Puis-je utiliser mon propre certificat?**

R: Oui:
```bash
# Cr√©er un secret TLS
kubectl create secret tls portfolio-tls \
  --cert=path/to/cert.crt \
  --key=path/to/cert.key

# Retirer l'annotation cert-manager de l'Ingress
kubectl annotate ingress portfolio-ingress cert-manager.io/cluster-issuer-
```

---

## Ressources Suppl√©mentaires

### Documentation Officielle

- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [kubectl Cheat Sheet](https://kubernetes.io/docs/reference/kubectl/cheatsheet/)
- [Lens Documentation](https://docs.k8slens.dev/)
- [cert-manager Documentation](https://cert-manager.io/docs/)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)
- [Docker Documentation](https://docs.docker.com/)

### Outils Utiles

- **kubectl**: CLI Kubernetes
- **Lens**: Interface graphique Kubernetes
- **k9s**: Terminal UI pour Kubernetes
- **stern**: Multi-pod log tailing
- **kubectx/kubens**: Changer de contexte/namespace rapidement

### Commandes d'Installation

```bash
# kubectl (si pas d√©j√† install√©)
# macOS
brew install kubectl

# Linux
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# Lens
# T√©l√©charger depuis https://k8slens.dev

# k9s (optionnel)
brew install k9s

# stern (optionnel)
brew install stern

# kubectx/kubens (optionnel)
brew install kubectx
```

---

**Derni√®re mise √† jour**: D√©cembre 2024  
**Version**: 1.0.0

**Pour toute question ou probl√®me non couvert dans ce guide, consulter**:
- [Guide de D√©ploiement](./DEPLOYMENT.md)
- [Documentation du projet](../README.md)
- Issues GitHub du projetnamespace: staging/' | \
  kubectl apply -f -

# 6. D√©ployer avec le nouveau tag
kubectl apply -f k8s/ -n staging
kubectl set image deployment/portfolio portfolio=ghcr.io/username/portfolio:feature-branch -n staging

# 7. V√©rifier
kubectl get pods -n staging
```

### Mise √† Jour des Manifests Kubernetes

```bash
# Appliquer tous les manifests
kubectl apply -f k8s/

# Appliquer un manifest sp√©cifique
kubectl apply -f k8s/deployment.yaml

# Dry-run pour v√©rifier sans appliquer
kubectl apply -f k8s/ --dry-run=client

# Voir les diff√©rences avant d'appliquer
kubectl diff -f k8s/

# Forcer le remplacement
kubectl replace --force -f k8s/deployment.yaml
```

---

## Proc√©dures de Rollback

### Rollback Automatique

Kubernetes effectue un rollback automatique si:
- Les nouveaux pods ne passent pas les health checks
- Le deployment ne progresse pas apr√®s `progressDeadlineSeconds` (600s par d√©faut)

**V√©rification**:
```bash
# Voir si un rollback automatique a eu lieu
kubectl describe deployment portfolio | grep -A 10 Conditions
```

### Rollback Manuel via kubectl

```bash
# ============================================
# M√âTHODE 1: Rollback √† la r√©vision pr√©c√©dente
# ============================================

# Annuler le dernier d√©ploiement
kubectl rollout undo deployment/portfolio

# V√©rifier le status
kubectl rollout status deployment/portfolio

# ============================================
# M√âTHODE 2: Rollback √† une r√©vision sp√©cifique
# ============================================

# 1. Voir l'historique des d√©ploiements
kubectl rollout history deployment/portfolio

# Sortie exemple:
# REVISION  CHANGE-CAUSE
# 1         <none>
# 2         Image updated to sha-abc123
# 3         Image updated to sha-def456

# 2. Voir les d√©tails d'une r√©vision
kubectl rollout history deployment/portfolio --revision=2

# 3. Rollback √† cette r√©vision
kubectl rollout undo deployment/portfolio --to-revision=2

# 4. V√©rifier
kubectl rollout status deployment/portfolio
kubectl get pods -l app=portfolio

# ============================================
# M√âTHODE 3: Rollback via image tag
# ============================================

# 1. Lister les images disponibles dans le registry
# (via l'interface web du registry)

# 2. D√©ployer une version sp√©cifique
kubectl set image deployment/portfolio \
  portfolio=ghcr.io/username/portfolio:sha-abc123

# 3. V√©rifier
kubectl rollout status deployment/portfolio
```

### Rollback via Lens

**Proc√©dure visuelle**:

1. **Ouvrir Lens**
2. **Naviguer vers Workloads ‚Üí Deployments**
3. **Cliquer sur `portfolio`**
4. **Onglet "Revisions"**
   - Voir toutes les r√©visions avec timestamps
   - Voir les changements de chaque r√©vision
5. **S√©lectionner une r√©vision pr√©c√©dente**
6. **Cliquer sur "Rollback to this revision"**
7. **Confirmer**
8. **V√©rifier dans l'onglet "Pods"** que les nouveaux pods d√©marrent

**Avantages de Lens**:
- Interface visuelle claire
- Voir les diff√©rences entre r√©visions
- Rollback en un clic
- Monitoring en temps r√©el du rollback

### Rollback via GitHub Actions Workflow

Si vous avez cr√©√© le workflow de rollback (task 5):

```bash
# 1. Aller sur GitHub ‚Üí Actions
# 2. S√©lectionner le workflow "Rollback"
# 3. Cliquer sur "Run workflow"
# 4. Entrer le tag de l'image √† d√©ployer (ex: sha-abc123)
# 5. Cliquer sur "Run workflow"
# 6. Suivre l'ex√©cution
```

**Ou via GitHub CLI**:

```bash
# D√©clencher le workflow de rollback
gh workflow run rollback.yml -f image_tag=sha-abc123

# Suivre l'ex√©cution
gh run watch
```

### Rollback d'Urgence (Downtime Acceptable)

Si vous devez rollback imm√©diatement et acceptez un court downtime:

```bash
# 1. Supprimer le deployment actuel
kubectl delete deployment portfolio

# 2. R√©appliquer avec l'ancienne version
# √âditer k8s/deployment.yaml pour changer l'image tag
nano k8s/deployment.yaml

# 3. R√©appliquer
kubectl apply -f k8s/deployment.yaml

# 4. V√©rifier
kubectl get pods -l app=portfolio
```

### V√©rification Apr√®s Rollback

```bash
# 1. V√©rifier que tous les pods sont Running et Ready
kubectl get pods -l app=portfolio

# Sortie attendue:
# NAME                         READY   STATUS    RESTARTS   AGE
# portfolio-xxx                1/1     Running   0          2m
# portfolio-yyy                1/1     Running   0          2m
# portfolio-zzz                1/1     Running   0          2m

# 2. V√©rifier les logs
kubectl logs -l app=portfolio --tail=20

# 3. Tester l'endpoint de health
kubectl port-forward service/portfolio-service 8080:80
curl http://localhost:8080/api/health

# 4. T-

tiques

--nalit√©s critionles fonc- Tester 
   ans Lenstriques d les m√©ifier- V√©res
   -10 minutdant 5s penes log V√©rifier l*
   -ploiement* chaque d√©orer apr√®s
5. **Monitloyer
nt de d√©pnt avater localeme- Tes
   nne toujours` fonctioealthe `/api/hassurer qu   - S'h checks**
ealtes h*Tester l ```

4. *123"
   # bug3 - Fix 1.2.on versi"Deploye-cause=s.io/changbernete
     ku \oliotfpornt  deploymennotatekubectl as
   d√©ploiementnnoter les    # Abash
``nts**
   `oiemes d√©ple dehistoriqu*Garder un  *)

3.(ambiguest`   - ‚ùå `lattra√ßable)
 23` (bc1 - ‚úÖ `sha-a*
  ites*icimage expls tags d'iser de
2. **Util
   ```
00 test00:30 run -p 30ckerest .
   douild -t t   docker b
 run build   npmm test
bash
   npr**
   ```√©ployeant de dter av tesoujours*T. ***:

1 pratiquesesck

**Bonne Rollbaes des Probl√®mntion d### Pr√©ve
les
```
 norma- M√©triqueserreur
# nts d'v√©nemes d'√©
# - Pavertsods ous les ps
# - Tdans Len V√©rifier 
# 5.pi/health
maine.com/a://votre-dol httpsduction
curester en pro